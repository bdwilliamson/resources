# Research resources

Here is a list of helpful papers and other resources for getting started working with me.

# Reading papers

Reading scientific papers can be hard! Here are a couple of resources for how to prioritize your read-throughs of papers (hint: you shouldn't necessarily just read the paper straight through the first time!).

* [The Leek group guide to reading papers](https://github.com/jtleek/readingpapers), a fairly comprehensive guide with lots of places to start reading
* [S. Keshav's guide to reading papers](https://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf), which I have found especially helpful for conference and arXiv papers

I suggest getting started by setting a daily alert on [arXiv](https://arxiv.org/), an open-access archive for scholarly articles; I personally have an alert set for the following categories: `stat.CO` (computation), `stat.ME` (methodology), `stat.ML` (machine learning), `stat.TH` (statistics theory). You can also set up alerts on [bioRxiv](https://www.biorxiv.org/) and [medRxiv](https://www.medrxiv.org/).

# Background on (generalized) linear regression

* [Chapter 1]() of Biostat 311, taught at the University of Washington in 2018 by myself and [Kelsey Grinde](https://kegrinde.github.io/). These slides cover univariate linear regression.
* [Chapter 2]() of Biostat 311. These slides cover multivariate linear regression.
* [Chapter 3]() of Biostat 311. These slides cover generalized linear regression.

# Background on penalized regression

* [ridge regression]()
* [the lasso](https://www.jstor.org/stable/pdf/2346178.pdf?refreqid=excelsior%3Ae393275b802f8fc0c2c0125e453694c1): pairs a sparsity-inducing penalty with a least-squares loss function, and is widely used
* more to come...

# Background on more flexible regression

* [classification and regression trees](https://www.eecis.udel.edu/~shatkay/Course/papers/CART2011.pdf): a simple-yet-flexible approach to regression
* [generalized additive models]()
* [random forests](https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf): build on regression/classification trees by building *forests* of multiple (bagged) trees
* [Super Learner](https://biostats.bepress.com/cgi/viewcontent.cgi?article=1226&context=ucbbiostat): combine the predictions from multiple *candidate learners* together to make better predictions
* more to come...

# Background on nonparametric and robust statistics

* more to come...

# Background on software

## Learning R

* https://education.rstudio.com/learn/
* https://rstudio.cloud/learn/primers
* https://swirlstats.com/
* https://adv-r.hadley.nz/ (for a bit more advanced treatment)
* https://r4ds.had.co.nz/ (for more of a data-science-y treatment)

## Learning Git and GitHub

* https://happygitwithr.com/
* https://git-scm.com/book/en/v2/ (especially [this section](https://git-scm.com/book/en/v2/Customizing-Git-Git-Configuration), which talks about configuring programs for commit message editing and templates)
* Tips for [git commit messages](https://chris.beams.io/posts/git-commit/)
* Tips for [how often to commit](https://www.freshconsulting.com/atomic-commits/)

## Data and packages

* [Tidy data](http://vita.had.co.nz/papers/tidy-data.pdf), a nice framework for organizing datasets described by Hadley Wickham
* [The Leek group guide to data sharing](https://github.com/jtleek/datasharing), a nice framework for organizing data that you are working on
* [The Leek group guide to creating R packages](https://github.com/jtleek/rpackages) (has links to other resources as well)
